"use strict";
Object.defineProperty(exports, "__esModule", {
    value: true
});
function _export(target, all) {
    for(var name in all)Object.defineProperty(target, name, {
        enumerable: true,
        get: all[name]
    });
}
_export(exports, {
    ClipEmbedding: function() {
        return ClipEmbedding;
    },
    ClipEmbeddingModelType: function() {
        return ClipEmbeddingModelType;
    }
});
const _lodash = /*#__PURE__*/ _interop_require_default(require("lodash"));
const _transformers = require("../internal/deps/transformers.js");
const _MultiModalEmbedding = require("./MultiModalEmbedding.js");
function _interop_require_default(obj) {
    return obj && obj.__esModule ? obj : {
        default: obj
    };
}
async function readImage(input) {
    const { RawImage } = await (0, _transformers.lazyLoadTransformers)();
    if (input instanceof Blob) {
        return await RawImage.fromBlob(input);
    } else if (_lodash.default.isString(input) || input instanceof URL) {
        return await RawImage.fromURL(input);
    } else {
        throw new Error(`Unsupported input type: ${typeof input}`);
    }
}
var ClipEmbeddingModelType;
(function(ClipEmbeddingModelType) {
    ClipEmbeddingModelType["XENOVA_CLIP_VIT_BASE_PATCH32"] = "Xenova/clip-vit-base-patch32";
    ClipEmbeddingModelType["XENOVA_CLIP_VIT_BASE_PATCH16"] = "Xenova/clip-vit-base-patch16";
})(ClipEmbeddingModelType || (ClipEmbeddingModelType = {}));
class ClipEmbedding extends _MultiModalEmbedding.MultiModalEmbedding {
    modelType = "Xenova/clip-vit-base-patch16";
    tokenizer = null;
    processor = null;
    visionModel = null;
    textModel = null;
    async getTokenizer() {
        const { AutoTokenizer } = await (0, _transformers.lazyLoadTransformers)();
        if (!this.tokenizer) {
            this.tokenizer = await AutoTokenizer.from_pretrained(this.modelType);
        }
        return this.tokenizer;
    }
    async getProcessor() {
        const { AutoProcessor } = await (0, _transformers.lazyLoadTransformers)();
        if (!this.processor) {
            this.processor = await AutoProcessor.from_pretrained(this.modelType);
        }
        return this.processor;
    }
    async getVisionModel() {
        const { CLIPVisionModelWithProjection } = await (0, _transformers.lazyLoadTransformers)();
        if (!this.visionModel) {
            this.visionModel = await CLIPVisionModelWithProjection.from_pretrained(this.modelType);
        }
        return this.visionModel;
    }
    async getTextModel() {
        const { CLIPTextModelWithProjection } = await (0, _transformers.lazyLoadTransformers)();
        if (!this.textModel) {
            this.textModel = await CLIPTextModelWithProjection.from_pretrained(this.modelType);
        }
        return this.textModel;
    }
    async getImageEmbedding(image) {
        const loadedImage = await readImage(image);
        const imageInputs = await (await this.getProcessor())(loadedImage);
        const { image_embeds } = await (await this.getVisionModel())(imageInputs);
        return Array.from(image_embeds.data);
    }
    async getTextEmbedding(text) {
        const textInputs = await (await this.getTokenizer())([
            text
        ], {
            padding: true,
            truncation: true
        });
        const { text_embeds } = await (await this.getTextModel())(textInputs);
        return text_embeds.data;
    }
}
