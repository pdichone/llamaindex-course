import type OpenAILLM from "openai";
import type { ClientOptions, ClientOptions as OpenAIClientOptions } from "openai";
import { OpenAI as OrigOpenAI } from "openai";
import { type BaseTool, type ChatMessage, type ChatResponse, type ChatResponseChunk, type LLM, type LLMChatParamsNonStreaming, type LLMChatParamsStreaming, type LLMMetadata, type MessageType, ToolCallLLM, type ToolCallLLMMessageOptions } from "@llamaindex/core/llms";
import type { ChatCompletionRole, ChatCompletionTool } from "openai/resources/chat/completions";
import type { ChatCompletionMessageParam } from "openai/resources/index.js";
import type { AzureOpenAIConfig } from "./azure.js";
export declare class OpenAISession {
    openai: Pick<OrigOpenAI, "chat" | "embeddings">;
    constructor(options?: ClientOptions & {
        azure?: boolean;
    });
}
/**
 * Get a session for the OpenAI API. If one already exists with the same options,
 * it will be returned. Otherwise, a new session will be created.
 * @param options
 * @returns
 */
export declare function getOpenAISession(options?: ClientOptions & {
    azure?: boolean;
}): OpenAISession;
export declare const GPT4_MODELS: {
    "gpt-4": {
        contextWindow: number;
    };
    "gpt-4-32k": {
        contextWindow: number;
    };
    "gpt-4-32k-0613": {
        contextWindow: number;
    };
    "gpt-4-turbo": {
        contextWindow: number;
    };
    "gpt-4-turbo-preview": {
        contextWindow: number;
    };
    "gpt-4-1106-preview": {
        contextWindow: number;
    };
    "gpt-4-0125-preview": {
        contextWindow: number;
    };
    "gpt-4-vision-preview": {
        contextWindow: number;
    };
    "gpt-4o": {
        contextWindow: number;
    };
    "gpt-4o-2024-05-13": {
        contextWindow: number;
    };
    "gpt-4o-mini": {
        contextWindow: number;
    };
    "gpt-4o-mini-2024-07-18": {
        contextWindow: number;
    };
};
export declare const GPT35_MODELS: {
    "gpt-3.5-turbo": {
        contextWindow: number;
    };
    "gpt-3.5-turbo-0613": {
        contextWindow: number;
    };
    "gpt-3.5-turbo-16k": {
        contextWindow: number;
    };
    "gpt-3.5-turbo-16k-0613": {
        contextWindow: number;
    };
    "gpt-3.5-turbo-1106": {
        contextWindow: number;
    };
    "gpt-3.5-turbo-0125": {
        contextWindow: number;
    };
};
/**
 * We currently support GPT-3.5 and GPT-4 models
 */
export declare const ALL_AVAILABLE_OPENAI_MODELS: {
    "gpt-3.5-turbo": {
        contextWindow: number;
    };
    "gpt-3.5-turbo-0613": {
        contextWindow: number;
    };
    "gpt-3.5-turbo-16k": {
        contextWindow: number;
    };
    "gpt-3.5-turbo-16k-0613": {
        contextWindow: number;
    };
    "gpt-3.5-turbo-1106": {
        contextWindow: number;
    };
    "gpt-3.5-turbo-0125": {
        contextWindow: number;
    };
    "gpt-4": {
        contextWindow: number;
    };
    "gpt-4-32k": {
        contextWindow: number;
    };
    "gpt-4-32k-0613": {
        contextWindow: number;
    };
    "gpt-4-turbo": {
        contextWindow: number;
    };
    "gpt-4-turbo-preview": {
        contextWindow: number;
    };
    "gpt-4-1106-preview": {
        contextWindow: number;
    };
    "gpt-4-0125-preview": {
        contextWindow: number;
    };
    "gpt-4-vision-preview": {
        contextWindow: number;
    };
    "gpt-4o": {
        contextWindow: number;
    };
    "gpt-4o-2024-05-13": {
        contextWindow: number;
    };
    "gpt-4o-mini": {
        contextWindow: number;
    };
    "gpt-4o-mini-2024-07-18": {
        contextWindow: number;
    };
};
export declare function isFunctionCallingModel(llm: LLM): llm is OpenAI;
export type OpenAIAdditionalMetadata = {};
export type OpenAIAdditionalChatOptions = Omit<Partial<OpenAILLM.Chat.ChatCompletionCreateParams>, "max_tokens" | "messages" | "model" | "temperature" | "top_p" | "stream" | "tools" | "toolChoice">;
export declare class OpenAI extends ToolCallLLM<OpenAIAdditionalChatOptions> {
    model: keyof typeof ALL_AVAILABLE_OPENAI_MODELS | string;
    temperature: number;
    topP: number;
    maxTokens?: number;
    additionalChatOptions?: OpenAIAdditionalChatOptions;
    apiKey?: string;
    maxRetries: number;
    timeout?: number;
    session: OpenAISession;
    additionalSessionOptions?: Omit<Partial<OpenAIClientOptions>, "apiKey" | "maxRetries" | "timeout">;
    constructor(init?: Partial<OpenAI> & {
        azure?: AzureOpenAIConfig;
    });
    get supportToolCall(): boolean;
    get metadata(): LLMMetadata & OpenAIAdditionalMetadata;
    static toOpenAIRole(messageType: MessageType): ChatCompletionRole;
    static toOpenAIMessage(messages: ChatMessage<ToolCallLLMMessageOptions>[]): ChatCompletionMessageParam[];
    chat(params: LLMChatParamsStreaming<OpenAIAdditionalChatOptions, ToolCallLLMMessageOptions>): Promise<AsyncIterable<ChatResponseChunk<ToolCallLLMMessageOptions>>>;
    chat(params: LLMChatParamsNonStreaming<OpenAIAdditionalChatOptions, ToolCallLLMMessageOptions>): Promise<ChatResponse<ToolCallLLMMessageOptions>>;
    protected streamChat(baseRequestParams: OpenAILLM.Chat.ChatCompletionCreateParams): AsyncIterable<ChatResponseChunk<ToolCallLLMMessageOptions>>;
    static toTool(tool: BaseTool): ChatCompletionTool;
}
