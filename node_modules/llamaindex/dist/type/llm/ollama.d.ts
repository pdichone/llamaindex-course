import { BaseEmbedding } from "@llamaindex/core/embeddings";
import type { ChatResponse, ChatResponseChunk, CompletionResponse, LLM, LLMChatParamsNonStreaming, LLMChatParamsStreaming, LLMCompletionParamsNonStreaming, LLMCompletionParamsStreaming, LLMMetadata } from "@llamaindex/core/llms";
import { Ollama as OllamaBase, type Config, type CopyRequest, type CreateRequest, type DeleteRequest, type EmbeddingsRequest, type EmbeddingsResponse, type GenerateRequest, type ListResponse, type GenerateResponse as OllamaGenerateResponse, type Options, type ProgressResponse, type PullRequest, type PushRequest, type ShowRequest, type ShowResponse, type StatusResponse } from "../internal/deps/ollama.js";
export type OllamaParams = {
    model: string;
    config?: Partial<Config>;
    options?: Partial<Options>;
};
/**
 * This class both implements the LLM and Embedding interfaces.
 */
export declare class Ollama extends BaseEmbedding implements LLM, Omit<OllamaBase, "chat"> {
    readonly hasStreaming = true;
    ollama: OllamaBase;
    model: string;
    options: Partial<Omit<Options, "num_ctx" | "top_p" | "temperature">> & Pick<Options, "num_ctx" | "top_p" | "temperature">;
    constructor(params: OllamaParams);
    get metadata(): LLMMetadata;
    chat(params: LLMChatParamsStreaming): Promise<AsyncIterable<ChatResponseChunk>>;
    chat(params: LLMChatParamsNonStreaming): Promise<ChatResponse>;
    complete(params: LLMCompletionParamsStreaming): Promise<AsyncIterable<CompletionResponse>>;
    complete(params: LLMCompletionParamsNonStreaming): Promise<CompletionResponse>;
    private getEmbedding;
    getTextEmbedding(text: string): Promise<number[]>;
    push(request: PushRequest & {
        stream: true;
    }): Promise<AsyncGenerator<ProgressResponse, any, unknown>>;
    push(request: PushRequest & {
        stream?: false | undefined;
    }): Promise<ProgressResponse>;
    abort(): void;
    encodeImage(image: string | Uint8Array): Promise<string>;
    generate(request: GenerateRequest & {
        stream: true;
    }): Promise<AsyncGenerator<OllamaGenerateResponse>>;
    generate(request: GenerateRequest & {
        stream?: false | undefined;
    }): Promise<OllamaGenerateResponse>;
    create(request: CreateRequest & {
        stream: true;
    }): Promise<AsyncGenerator<ProgressResponse>>;
    create(request: CreateRequest & {
        stream?: false | undefined;
    }): Promise<ProgressResponse>;
    pull(request: PullRequest & {
        stream: true;
    }): Promise<AsyncGenerator<ProgressResponse>>;
    pull(request: PullRequest & {
        stream?: false | undefined;
    }): Promise<ProgressResponse>;
    delete(request: DeleteRequest): Promise<StatusResponse>;
    copy(request: CopyRequest): Promise<StatusResponse>;
    list(): Promise<ListResponse>;
    show(request: ShowRequest): Promise<ShowResponse>;
    embeddings(request: EmbeddingsRequest): Promise<EmbeddingsResponse>;
}
