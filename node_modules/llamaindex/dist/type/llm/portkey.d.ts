import { BaseLLM, type ChatMessage, type ChatResponse, type ChatResponseChunk, type LLMChatParamsNonStreaming, type LLMChatParamsStreaming, type LLMMetadata } from "@llamaindex/core/llms";
import type { LLMOptions } from "portkey-ai";
import { Portkey as OrigPortKey } from "portkey-ai";
interface PortkeyOptions {
    apiKey?: string;
    baseURL?: string;
    mode?: string;
    llms?: [LLMOptions] | null;
}
export declare class PortkeySession {
    portkey: OrigPortKey;
    constructor(options?: PortkeyOptions);
}
/**
 * Get a session for the Portkey API. If one already exists with the same options,
 * it will be returned. Otherwise, a new session will be created.
 * @param options
 * @returns
 */
export declare function getPortkeySession(options?: PortkeyOptions): PortkeySession;
export declare class Portkey extends BaseLLM {
    apiKey?: string;
    baseURL?: string;
    mode?: string;
    llms?: [LLMOptions] | null;
    session: PortkeySession;
    constructor(init?: Partial<Portkey>);
    get metadata(): LLMMetadata;
    chat(params: LLMChatParamsStreaming): Promise<AsyncIterable<ChatResponseChunk>>;
    chat(params: LLMChatParamsNonStreaming): Promise<ChatResponse>;
    streamChat(messages: ChatMessage[], params?: Record<string, any>): AsyncIterable<ChatResponseChunk>;
}
export {};
