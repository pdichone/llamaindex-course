import { BaseLLM, type ChatResponse, type ChatResponseChunk, type LLMChatParamsNonStreaming, type LLMChatParamsStreaming } from "@llamaindex/core/llms";
export declare const ALL_AVAILABLE_MISTRAL_MODELS: {
    "mistral-tiny": {
        contextWindow: number;
    };
    "mistral-small": {
        contextWindow: number;
    };
    "mistral-medium": {
        contextWindow: number;
    };
};
export declare class MistralAISession {
    apiKey?: string;
    private client;
    constructor(init?: Partial<MistralAISession>);
    getClient(): Promise<any>;
}
/**
 * MistralAI LLM implementation
 */
export declare class MistralAI extends BaseLLM {
    model: keyof typeof ALL_AVAILABLE_MISTRAL_MODELS;
    temperature: number;
    topP: number;
    maxTokens?: number;
    apiKey?: string;
    safeMode: boolean;
    randomSeed?: number;
    private session;
    constructor(init?: Partial<MistralAI>);
    get metadata(): {
        model: "mistral-tiny" | "mistral-small" | "mistral-medium";
        temperature: number;
        topP: number;
        maxTokens: number | undefined;
        contextWindow: number;
        tokenizer: undefined;
    };
    private buildParams;
    chat(params: LLMChatParamsStreaming): Promise<AsyncIterable<ChatResponseChunk>>;
    chat(params: LLMChatParamsNonStreaming): Promise<ChatResponse>;
    protected streamChat({ messages, }: LLMChatParamsStreaming): AsyncIterable<ChatResponseChunk>;
}
