import type { ChatMessage, LLM } from "@llamaindex/core/llms";
import type { EngineResponse } from "@llamaindex/core/schema";
import type { ChatHistory } from "../../ChatHistory.js";
import type { CondenseQuestionPrompt } from "../../Prompt.js";
import type { ServiceContext } from "../../ServiceContext.js";
import { PromptMixin } from "../../prompts/index.js";
import type { QueryEngine } from "../../types.js";
import type { ChatEngine, ChatEngineParamsNonStreaming, ChatEngineParamsStreaming } from "./types.js";
/**
 * CondenseQuestionChatEngine is used in conjunction with a Index (for example VectorStoreIndex).
 * It does two steps on taking a user's chat message: first, it condenses the chat message
 * with the previous chat history into a question with more context.
 * Then, it queries the underlying Index using the new question with context and returns
 * the response.
 * CondenseQuestionChatEngine performs well when the input is primarily questions about the
 * underlying data. It performs less well when the chat messages are not questions about the
 * data, or are very referential to previous context.
 */
export declare class CondenseQuestionChatEngine extends PromptMixin implements ChatEngine {
    queryEngine: QueryEngine;
    chatHistory: ChatHistory;
    llm: LLM;
    condenseMessagePrompt: CondenseQuestionPrompt;
    constructor(init: {
        queryEngine: QueryEngine;
        chatHistory: ChatMessage[];
        serviceContext?: ServiceContext;
        condenseMessagePrompt?: CondenseQuestionPrompt;
    });
    protected _getPrompts(): {
        condenseMessagePrompt: CondenseQuestionPrompt;
    };
    protected _updatePrompts(promptsDict: {
        condenseMessagePrompt: CondenseQuestionPrompt;
    }): void;
    private condenseQuestion;
    chat(params: ChatEngineParamsStreaming): Promise<AsyncIterable<EngineResponse>>;
    chat(params: ChatEngineParamsNonStreaming): Promise<EngineResponse>;
    reset(): void;
}
