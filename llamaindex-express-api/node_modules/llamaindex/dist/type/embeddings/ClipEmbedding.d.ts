import type { ImageType } from "@llamaindex/core/schema";
import { MultiModalEmbedding } from "./MultiModalEmbedding.js";
import type { CLIPTextModelWithProjection, CLIPVisionModelWithProjection, PreTrainedTokenizer, Processor } from "@xenova/transformers";
export declare enum ClipEmbeddingModelType {
    XENOVA_CLIP_VIT_BASE_PATCH32 = "Xenova/clip-vit-base-patch32",
    XENOVA_CLIP_VIT_BASE_PATCH16 = "Xenova/clip-vit-base-patch16"
}
export declare class ClipEmbedding extends MultiModalEmbedding {
    modelType: ClipEmbeddingModelType;
    private tokenizer;
    private processor;
    private visionModel;
    private textModel;
    getTokenizer(): Promise<PreTrainedTokenizer>;
    getProcessor(): Promise<Processor>;
    getVisionModel(): Promise<CLIPVisionModelWithProjection>;
    getTextModel(): Promise<CLIPTextModelWithProjection>;
    getImageEmbedding(image: ImageType): Promise<number[]>;
    getTextEmbedding(text: string): Promise<number[]>;
}
