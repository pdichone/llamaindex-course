"use strict";
Object.defineProperty(exports, "__esModule", {
    value: true
});
Object.defineProperty(exports, "Ollama", {
    enumerable: true,
    get: function() {
        return Ollama;
    }
});
const _embeddings = require("@llamaindex/core/embeddings");
const _utils = require("@llamaindex/core/utils");
const _ollama = require("../internal/deps/ollama.js");
const messageAccessor = (part)=>{
    return {
        raw: part,
        delta: part.message.content
    };
};
const completionAccessor = (part)=>{
    return {
        text: part.response,
        raw: part
    };
};
class Ollama extends _embeddings.BaseEmbedding {
    hasStreaming = true;
    ollama;
    // https://ollama.ai/library
    model;
    options = {
        num_ctx: 4096,
        top_p: 0.9,
        temperature: 0.7
    };
    constructor(params){
        super();
        this.model = params.model;
        this.ollama = new _ollama.Ollama(params.config);
        if (params.options) {
            this.options = {
                ...this.options,
                ...params.options
            };
        }
    }
    get metadata() {
        const { temperature, top_p, num_ctx } = this.options;
        return {
            model: this.model,
            temperature: temperature,
            topP: top_p,
            maxTokens: undefined,
            contextWindow: num_ctx,
            tokenizer: undefined
        };
    }
    async chat(params) {
        const { messages, stream } = params;
        const payload = {
            model: this.model,
            messages: messages.map((message)=>({
                    role: message.role,
                    content: (0, _utils.extractText)(message.content)
                })),
            stream: !!stream,
            options: {
                ...this.options
            }
        };
        if (!stream) {
            const chatResponse = await this.ollama.chat({
                ...payload,
                stream: false
            });
            return {
                message: {
                    role: "assistant",
                    content: chatResponse.message.content
                },
                raw: chatResponse
            };
        } else {
            const stream = await this.ollama.chat({
                ...payload,
                stream: true
            });
            return (0, _utils.streamConverter)(stream, messageAccessor);
        }
    }
    async complete(params) {
        const { prompt, stream } = params;
        const payload = {
            model: this.model,
            prompt: (0, _utils.extractText)(prompt),
            stream: !!stream,
            options: {
                ...this.options
            }
        };
        if (!stream) {
            const response = await this.ollama.generate({
                ...payload,
                stream: false
            });
            return {
                text: response.response,
                raw: response
            };
        } else {
            const stream = await this.ollama.generate({
                ...payload,
                stream: true
            });
            return (0, _utils.streamConverter)(stream, completionAccessor);
        }
    }
    async getEmbedding(prompt) {
        const payload = {
            model: this.model,
            prompt,
            options: {
                ...this.options
            }
        };
        const response = await this.ollama.embeddings({
            ...payload
        });
        return response.embedding;
    }
    async getTextEmbedding(text) {
        return this.getEmbedding(text);
    }
    push(request) {
        return this.ollama.push(request);
    }
    abort() {
        return this.ollama.abort();
    }
    encodeImage(image) {
        return this.ollama.encodeImage(image);
    }
    generate(request) {
        return this.ollama.generate(request);
    }
    create(request) {
        return this.ollama.create(request);
    }
    pull(request) {
        return this.ollama.pull(request);
    }
    delete(request) {
        return this.ollama.delete(request);
    }
    copy(request) {
        return this.ollama.copy(request);
    }
    list() {
        return this.ollama.list();
    }
    show(request) {
        return this.ollama.show(request);
    }
    embeddings(request) {
        return this.ollama.embeddings(request);
    }
}
